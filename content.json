{"posts":[{"title":"论文阅读：《NaSh:Guardrails for an LLM-Powered Natural》","text":"Arxiv Paper: 《NaSh: Guardrails for an LLM-Powered NaturalLanguage Shell》 一、概要本文提出了一个问题：由LLM驱动的shell在架构上和传统的shell有什么区别？并给出了在设计LLM驱动的shell上的一些指导规范。 二、Shell工作流2-1 命令行交互命令行交互用来执行一些一次性（one-off）的操作，如列举文件或者执行一些系统命令，该操作可以看作四个步骤的轮回： Develop: 用户编写command Run: 用户运行command Inspection: 观察系统是否受到严重影响 Revert: 如果发现异常，回滚状态 从用户视角，命令行交互有以下挑战： 用户为了执行命令往往需要阅读文档学习一些外部工具的使用并且在遇到一些非常用的命令时需要操作验证 回滚需要用户检测并重新执行一些复杂的操作，且一旦回滚不充分会导致系统状态的不一致。 2.2 脚本脚本往往用来执行一些可复用(reusable)的操作，和命令行交互一样，用户同样需要进行上述四种操作的轮回，除此之外，编写脚本引入了新的挑战：用户需要保障脚本在未来执行的正确性。 2.3 LLM的机遇与挑战随着LLM的诞生和发展，用户可以越过学习的步骤，直接让LLM生成任务对应的command/script。 由于LLM无法保证其生成结果的正确性且用户并不直接参与到命令/脚本逻辑的设计中，这可能反而会加剧用户检查和回滚脚本的难度。比如脚本可能在非当前的工作目录下生成了文件，用户可能在很久后才会察觉。 即使LLMS生成正确的脚本/命令，但是自然语言本身具备的歧义性也会导致错误的执行结果。例如：“请帮我删除/log目录下的所有log文件”，那么歧义点在于/log目录下子目录的log文件是否需要删除呢？.system.log这种隐藏的log文件是否也需要删除呢？ 三、NaSh: 一个自然语言ShellNaSh引入了“代码生成”，“沙箱环境下的逆堆叠文件系统”，“影响总结”和“基于符号执行的测试用例生成”等方式来解决上述挑战。 3.1 代码生成NaSh设计中的核心问题是如何代表用户与系统交互。传统方法（如LLM代理直接执行系统调用、模拟图形界面操作）存在可解释性和可重复性缺陷，主要因为： 调试困难：若LLM对系统调用或点击操作响应错误，用户难以追踪问题根源 非确定性风险：LLM因技术限制可能对相同输入产生不同输出NaSh的创新方案是让LLM根据用户指令生成可分析、可执行的代码，其优势在于： 可分析性：利用程序分析技术帮助用户纠偏 确定性执行：生成的代码可稳定复现，不受LLM非确定性影响 开发友好性：代码可作为脚本复用，或由用户直接修改完善这一设计通过代码作为媒介实现了可解释性、可重复性和用户可控性的平衡。 3.2 沙箱环境为了避免LLM生成的代码的不确定性引入的风险，NaSh通过沙箱环境安全执行LLMs生成的代码，并提供事务型撤销（undo）机制。为了避免对文件系统的影响，NaSh进行了文件系统的隔离，然而传统的Overlay文件系统修改仅在提交后对其他应用可见且不适用于一些特殊文件（/proc），NaSh创新的引入了逆Overlay文件系统，直接修改操作系统文件但是可以回滚到旧版本（缺陷是对磁盘空间要求高）。 3.3 影响分析由于NaSh基于沙箱环境执行，因此可以监控脚本执行后的影响并基于LLM给出总结，该总结可以帮助LLM/用户来更好的判断系统所受到的影响以进行优化下个版本迭代。 总结个人感觉本文提出的一些方案还是偏理想化，不太好落地。","link":"/2025/06/21/Nash-2025-06-21/"},{"title":"从零到一的大模型之旅：一个大语言模型是如何工作的？","text":"What i cannot create, i do not understand 从2022年11月30日OpenAI发布了智能聊天对话应用ChatGPT以来，大模型在两年半的时间内几乎成为了人人尽知的词语。无论是领域前沿的研究人员，还是企业中的工程师，亦或是自媒体的从业人员，都或多或少的在主动使用大模型来提高自己的效率。凭借对知识的”压缩能力”，人们可以快速高效的获得自己想要得到的答案，人们对于知识获取的方式发生了巨大变革。 作为程序员最重要网站之一的Stack Overflow，在大模型诞生后，面临着“Stack overflow is almost dead”的趋势。 正如Richard Feynman所说：“What i cannot create, i do not understand”，了解大模型运行的基础原理总归是有益的，本文将从大模型的基础原理讲起，理清大模型“智能”背后的技术。 一、一切都是概率，预测下一个词一言以蔽之，大模型本质上是一个“单词补全器”，对于输入的一个单词序列，大模型会给出下一个可能的单词整体的概率分布。，不断的重复这个过程，大模型就能够产生出一段“有意义的”文本。如果大模型收到了下面的输入： 1I want to eat __ 他将会输出候选单词列表中每个单词作为答案的概率，比如： 123apple 0.7flower 0.2sky 0.1 大模型会选择概率最高的单词作为答案，此时用户输入的句子就会变为 1I want to eat apple. ChatGPT之所以能够根据用户的问题回答出对应的答案，本质上也是一种补全。一般情况下，基于大模型的聊天应用都会内置一个提前预制好的单词序列前缀，我们称之为System Prompt（系统提示词），这样在单次问答的场景下，大模型的输出本质上是在补全System Prompt和User System Prompt，通过这种补全机制，大模型便可以输出针对用户问题的答案。 二、向量化，确定一个单词的“语义”显然，计算机从诞生到现在只能用来处理“数字”，而我们输入的是各种自然语言，为了能够让计算机理解单词所对应的语义，我们需要建立起一座从单词到数字的桥梁，在深度学习领域中，这个桥梁叫做Embedding（向量化）。 向量化的目的是为每个单词在一个高维空间中寻找到该单词对应的向量。可以这么理解，每个单词拥有若干的特征，这些特征可以唯一的确认一个单词的含义。如果我们能可视化这个高维空间，我们可以看到意思相近的词语对应的向量挨得越近，如cat和dog，因为它们都是家养宠物。我们也可以通过单词向量之间的运算变换得到一些有意思的发现。比如： $ Vec(Germany) - Vec(Italy) + Vec(Hitler) \\approx Vec(Mussolini) $ 在GPT-3模型中，每个单词向量有12288个维度。当模型接收到一串单词序列后，便会对这个单词序列进行Embedding操作，得到大模型“实际”可以被识别的输入：一个$N * M$的矩阵，$N$为单词数目，$M$为词向量的维度。 三、如何确认上下文语境中的单词含义？Attention is All You Need！虽然向量化可以确定每个输入单词的含义，但是在不同的语境下，同样的单词却有着不同的含义，如何准确的获取每个单词在当前语境下”正确”的向量?从某种角度上讲，一旦能解决这个问题，计算机便可以“理解”句子的含义了。 2017年，Google在NIPS发表了一遍标题为《Attention Is All You Need》的论文，这篇论文通过引入了注意力机制成功的解决了在不同上下文环境下确认单词含义的挑战，这也成为了大模型技术发展的重要基石。 假设大模型输入了一段文本： 1There is a blue cat 在上述语境下，cat将不再是原本的cat，而会受到blue这个形容词的影响，那么这种“影响”是否可也被向量化呢？答案是Yes。在注意力机制中，对单词定义了一种查询操作$Query$，该操作可以理解为：“我前面有形容词吗？”或者“我前面有贬义词么”等一些语义上的讯问。 为了实现查询操作，注意力模块训练了一个矩阵$W_{q}$，将其与当前单词的向量进行点积操作即可得到当前单词的一个查询矩阵$Q$。作为$Query$的回应，注意力模块训练了另外一个矩阵$W_{k}$，前面的单词向量和$W_{k}$点积可以得到每个单词的$K$矩阵。将当前单词的$Q$和前面的每个单词的$K$进行点积，即可得到前面每个单词对当前单词的“影响力”。通过softmax方法归一化，就可以得到每个单词影响当前单词的概率。最终，为了将当前单词“校准到”真实含义的向量，注意力引入了第三个矩阵$W_{v}$，前面的每个单词和$W_{v}$矩阵进行点积后便得到了向量$V$。形式化的，注意力机制可以用如下公式计算： $Attention(Q, K, V) = softmax(\\frac{QK^{T}}{\\sqrt{d_k}})V$ 当计算完当前单词与前面每个单词的注意力后，将当前单词的的向量累加上注意力向量即可得到校准后的向量，换句话说，通过atttion模块计算后，单词的向量编码了更为丰富的含义。 上述描述的计算过程被称为“单头注意力机制”，在Google提出的论文中，Transformer架构采用了并行的多头注意力机制，也即对于每个单词并行的运行多次注意力，然后加权平均得到最终的校准后的向量。 四、Transformer！最终我们得到了大模型的基础架构：Transformer 回过头再看Transformer架构就能发现，序列首先被Embedding，然后经过多层的Attention模块不断强化单词语义，需要注意的是，Transformer在训练过程中采用了Masked Multi-Head Attention，这是为了避免在训练的过程中出现根据数据集后面的单词影响前面单词的情况，比如如果数据集是 1I want to eat _ and eggs 在预测空格部分时，应当只考虑I want to eat对空格位置的影响，所以强制将空格后续位置的Attention设置为0以避免这种影响。最终，Transformer输出下一个单词的概率分布，完成了大模型的“智能”过程。","link":"/2025/06/29/Transformer/"}],"tags":[],"categories":[],"pages":[]}